{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c691d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b282ed",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089804ff",
   "metadata": {},
   "source": [
    "### 1) 전처리: 데이터 로드\n",
    "요구사항\n",
    "* 12000개 이상 사전을 생성\n",
    "* 텐서 길이는 15를 넘지 않도록\n",
    "* 데이터상 필요없는 텍스트는 정제 할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3cc9acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\\n\", 'That David played, and it pleased the Lord\\n', \"But you don't really care for music, do you?\\n\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in glob.glob(txt_file_path):\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw_corpus.extend(f.readlines())\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd35f5",
   "metadata": {},
   "source": [
    "### 2) 전처리: 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6f9387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의 문장: This @_is ;;;sample        sentence.\n",
      "정제: <start> this is sample sentence . <end>\n",
      "----\n",
      "before: \"Now I've heard there was a secret chord\n",
      "\"\n",
      "after: \"<start> now i've heard there was a secret chord <end>\"\n",
      "----\n",
      "before: \"That David played, and it pleased the Lord\n",
      "\"\n",
      "after: \"<start> that david played , and it pleased the lord <end>\"\n",
      "----\n",
      "before: \"But you don't really care for music, do you?\n",
      "\"\n",
      "after: \"<start> but you don't really care for music , do you ? <end>\"\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # ?.!,¿ 기호 앞에 공백 추가\n",
    "    # 허용된 문자 외 공백 치환\n",
    "    # i'm i've 같은 영문축약표기도 범용\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence) # 하나 이상 중복된 공백(스페이스) 하나로 처리\n",
    "    return f\"<start> { sentence.strip() } <end>\" # start/end 마커 추가\n",
    "\n",
    "# 임의문장 테스트\n",
    "print(\"임의 문장: This @_is ;;;sample        sentence.\")\n",
    "print(\"정제:\", preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
    "print(\"----\")\n",
    "\n",
    "# 앞 단계에서 확인한 3개 문장 의도된 전처리여부 적용 확인\n",
    "for i in range(3):\n",
    "    print(f\"before: \\\"{raw_corpus[i]}\\\"\")\n",
    "    print(f\"after: \\\"{preprocess_sentence(raw_corpus[i])}\\\"\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbdf7d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<start> now i've heard there was a secret chord <end>\",\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " \"<start> but you don't really care for music , do you ? <end>\",\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split()) > 15 : continue  # 텐서 길이 15이상은 필터링\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859f7aa",
   "metadata": {},
   "source": [
    "> 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences 참고자료\n",
    ">\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    ">\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f71a8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서 길이: 15\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 12,500 단어장 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12500, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "print(\"텐서 길이:\", len(tensor[0]))\n",
    "assert len(tensor[0]) == 15  # 각 텐서길이 15개를 안넘도록 했으니"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df3ecff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : the\n",
      "6 : i\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f259a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   46  149  308   84   50    9  982 6534    3    0    0    0    0]\n",
      "[  46  149  308   84   50    9  982 6534    3    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc4695",
   "metadata": {},
   "source": [
    "> 데이터셋에 대해\n",
    ">\n",
    "> https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e479c",
   "metadata": {},
   "source": [
    "## 모델 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0276ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0594239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a950c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12501), dtype=float32, numpy=\n",
       "array([[[ 3.8211307e-05,  3.5074136e-05, -1.3654288e-04, ...,\n",
       "         -3.4924431e-04, -2.7331791e-04,  3.9411095e-04],\n",
       "        [ 9.0569614e-05, -2.1670219e-05, -2.6541881e-04, ...,\n",
       "         -6.7214982e-04, -4.0594189e-04,  7.0340326e-04],\n",
       "        [ 4.0644282e-04, -1.1582365e-04, -4.6537604e-04, ...,\n",
       "         -8.6144323e-04, -4.6995829e-04,  9.5705769e-04],\n",
       "        ...,\n",
       "        [-7.2658667e-04,  8.4651471e-04, -8.8273484e-04, ...,\n",
       "          1.5938977e-03,  1.4034158e-03,  1.0284520e-03],\n",
       "        [-7.6541345e-04,  1.0851120e-03, -1.0982318e-03, ...,\n",
       "          1.7432676e-03,  1.6627798e-03,  9.9626719e-04],\n",
       "        [-7.5790280e-04,  1.2855048e-03, -1.3104635e-03, ...,\n",
       "          1.8524878e-03,  1.8854543e-03,  9.4799150e-04]],\n",
       "\n",
       "       [[ 3.8211307e-05,  3.5074136e-05, -1.3654288e-04, ...,\n",
       "         -3.4924431e-04, -2.7331791e-04,  3.9411095e-04],\n",
       "        [-1.0655235e-04,  1.0084828e-04, -1.4812197e-04, ...,\n",
       "         -6.4449094e-04, -3.1683195e-04,  6.4687565e-04],\n",
       "        [-8.2076403e-05, -1.1946812e-04, -2.2392516e-04, ...,\n",
       "         -4.4775044e-04, -3.5800007e-05,  7.1432954e-04],\n",
       "        ...,\n",
       "        [-3.4574693e-06, -3.4478551e-04,  3.8642104e-04, ...,\n",
       "          1.5459997e-04,  1.3728117e-03,  1.0313812e-03],\n",
       "        [ 1.7015831e-04, -2.1815927e-04,  3.0548434e-04, ...,\n",
       "          9.2588001e-05,  1.3350230e-03,  1.2961479e-03],\n",
       "        [ 1.5051337e-04, -2.2240441e-04,  3.1538113e-04, ...,\n",
       "         -2.1488940e-04,  1.1224110e-03,  1.4426082e-03]],\n",
       "\n",
       "       [[ 3.8211307e-05,  3.5074136e-05, -1.3654288e-04, ...,\n",
       "         -3.4924431e-04, -2.7331791e-04,  3.9411095e-04],\n",
       "        [ 6.2736915e-05,  7.4894975e-05, -1.7108543e-04, ...,\n",
       "         -8.8082481e-04, -5.9669977e-04,  4.9925374e-04],\n",
       "        [-2.3127212e-05,  3.5845689e-04, -1.2736891e-04, ...,\n",
       "         -1.2171954e-03, -5.2035099e-04,  2.6626690e-04],\n",
       "        ...,\n",
       "        [-4.6460460e-05, -3.0498891e-04,  4.9712899e-04, ...,\n",
       "          1.1542453e-03,  6.9616042e-04,  6.7938428e-04],\n",
       "        [-2.5314273e-04, -7.4828829e-05,  4.0992250e-04, ...,\n",
       "          1.4186146e-03,  1.0787671e-03,  8.0588699e-04],\n",
       "        [-4.2280805e-04,  1.8371287e-04,  2.3171447e-04, ...,\n",
       "          1.6461628e-03,  1.4216390e-03,  9.0003066e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.8211307e-05,  3.5074136e-05, -1.3654288e-04, ...,\n",
       "         -3.4924431e-04, -2.7331791e-04,  3.9411095e-04],\n",
       "        [-1.0167609e-05,  1.7475552e-04, -8.3948355e-05, ...,\n",
       "         -3.4992839e-04, -2.4805195e-04,  5.2504829e-04],\n",
       "        [-5.3619649e-05,  3.8418395e-04,  4.2003550e-05, ...,\n",
       "         -1.1173760e-04, -1.0766506e-04,  4.9641001e-04],\n",
       "        ...,\n",
       "        [ 3.4662575e-04,  1.4802380e-03,  2.3804087e-04, ...,\n",
       "          2.0084204e-03,  3.0977494e-04, -1.2875130e-04],\n",
       "        [ 2.1881374e-04,  1.5722982e-03, -7.1110135e-06, ...,\n",
       "          2.1854418e-03,  5.3641270e-04, -4.6715199e-06],\n",
       "        [ 9.2861090e-05,  1.6602899e-03, -2.8740766e-04, ...,\n",
       "          2.3196265e-03,  7.9968042e-04,  1.1177236e-04]],\n",
       "\n",
       "       [[ 3.8211307e-05,  3.5074136e-05, -1.3654288e-04, ...,\n",
       "         -3.4924431e-04, -2.7331791e-04,  3.9411095e-04],\n",
       "        [-8.1812876e-05,  8.0206155e-05, -3.1303216e-04, ...,\n",
       "         -9.5074787e-04, -3.1830673e-04,  5.4912944e-04],\n",
       "        [ 2.8490959e-04, -1.0085863e-04, -3.1136323e-04, ...,\n",
       "         -1.3166002e-03, -2.9481217e-04,  5.6067237e-04],\n",
       "        ...,\n",
       "        [-8.9902768e-04, -1.2469281e-03, -6.5357417e-05, ...,\n",
       "         -3.9377867e-04, -6.1349099e-04,  2.9290828e-04],\n",
       "        [-1.0718702e-03, -9.5331477e-04, -2.3586814e-04, ...,\n",
       "         -8.6824955e-05, -2.7175000e-04,  3.6859480e-04],\n",
       "        [-1.2108119e-03, -5.6303450e-04, -4.6035732e-04, ...,\n",
       "          2.2662184e-04,  9.8934055e-05,  4.4894146e-04]],\n",
       "\n",
       "       [[ 3.8211307e-05,  3.5074136e-05, -1.3654288e-04, ...,\n",
       "         -3.4924431e-04, -2.7331791e-04,  3.9411095e-04],\n",
       "        [ 3.0279371e-05,  5.6874334e-05, -4.8028008e-04, ...,\n",
       "         -3.3777376e-04, -3.5148716e-04,  7.5759809e-04],\n",
       "        [ 2.0068184e-04,  6.5857152e-05, -6.3766778e-04, ...,\n",
       "         -1.0184616e-04, -4.8253485e-05,  7.9878612e-04],\n",
       "        ...,\n",
       "        [ 1.2824851e-03, -7.1736140e-04,  7.4799970e-04, ...,\n",
       "         -2.0188281e-04,  2.3023987e-03, -4.2422733e-05],\n",
       "        [ 1.3197433e-03, -7.8218611e-04,  8.8577811e-04, ...,\n",
       "         -2.2867486e-04,  2.4001277e-03, -1.9049202e-04],\n",
       "        [ 1.1669256e-03, -6.1709568e-04,  8.1102055e-04, ...,\n",
       "         -5.6718400e-05,  2.4481015e-03, -1.9183772e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e30603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3200256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12813525  \n",
      "=================================================================\n",
      "Total params: 29,653,461\n",
      "Trainable params: 29,653,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970e9fa",
   "metadata": {},
   "source": [
    "> optimizer와 loss등은 차차 배웁니다\n",
    ">\n",
    "> 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    ">\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    ">\n",
    "> https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    ">\n",
    "> 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d0663",
   "metadata": {},
   "source": [
    "## 모델 트레이닝 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4dd88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "663/663 [==============================] - 119s 177ms/step - loss: 3.2262\n",
      "Epoch 2/10\n",
      "663/663 [==============================] - 120s 181ms/step - loss: 2.8228\n",
      "Epoch 3/10\n",
      "663/663 [==============================] - 120s 180ms/step - loss: 2.6662\n",
      "Epoch 4/10\n",
      "663/663 [==============================] - 121s 181ms/step - loss: 2.5439\n",
      "Epoch 5/10\n",
      "663/663 [==============================] - 121s 182ms/step - loss: 2.4385\n",
      "Epoch 6/10\n",
      "663/663 [==============================] - 120s 181ms/step - loss: 2.3420\n",
      "Epoch 7/10\n",
      "663/663 [==============================] - 120s 180ms/step - loss: 2.2528\n",
      "Epoch 8/10\n",
      "663/663 [==============================] - 120s 181ms/step - loss: 2.1698\n",
      "Epoch 9/10\n",
      "663/663 [==============================] - 120s 181ms/step - loss: 2.0906\n",
      "Epoch 10/10\n",
      "663/663 [==============================] - 120s 181ms/step - loss: 2.0162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f713199ce20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True,\n",
    "    reduction = 'none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059453b",
   "metadata": {},
   "source": [
    "## 모델이 만들어 낸 가사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0facc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return re.sub(r\"(^<start>|<end>$)\", \"\", generated.strip()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9ece89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제시 키워드: i'm a boy\n",
      "만들어낸 가사: i'm a boy , i'm a bad girl\n"
     ]
    }
   ],
   "source": [
    "keyword = \"i'm a boy\"\n",
    "print(\"제시 키워드:\", keyword)\n",
    "print(\"만들어낸 가사:\", generate_text(model, tokenizer, init_sentence=f\"<start> {keyword}\", max_len=20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
